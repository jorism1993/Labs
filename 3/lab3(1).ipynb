{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9a10081f57b90a368eb8daf62e3ba00e",
     "grade": false,
     "grade_id": "cell-02487845739eb4fd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Lab 3: Expectation Maximization and Variational Autoencoder\n",
    "\n",
    "### Machine Learning 2 (2017/2018)\n",
    "\n",
    "* The lab exercises should be made in groups of two or three people.\n",
    "* The deadline is Friday, 01.06.\n",
    "* Assignment should be submitted through BlackBoard! Make sure to include your and your teammates' names with the submission.\n",
    "* Attach the .IPYNB (IPython Notebook) file containing your code and answers. Naming of the file should be \"studentid1\\_studentid2\\_lab#\", for example, the attached file should be \"12345\\_12346\\_lab1.ipynb\". Only use underscores (\"\\_\") to connect ids, otherwise the files cannot be parsed.\n",
    "\n",
    "Notes on implementation:\n",
    "\n",
    "* You should write your code and answers in an IPython Notebook: http://ipython.org/notebook.html. If you have problems, please ask.\n",
    "* Use __one cell__ for code and markdown answers only!\n",
    "    * Put all code in the cell with the ```# YOUR CODE HERE``` comment and overwrite the ```raise NotImplementedError()``` line.\n",
    "    * For theoretical questions, put your solution using LaTeX style formatting in the YOUR ANSWER HERE cell.\n",
    "* Among the first lines of your notebook should be \"%pylab inline\". This imports all required modules, and your plots will appear inline.\n",
    "* Large parts of you notebook will be graded automatically. Therefore it is important that your notebook can be run completely without errors and within a reasonable time limit. To test your notebook before submission, select Kernel -> Restart \\& Run All.\n",
    "$\\newcommand{\\bx}{\\mathbf{x}} \\newcommand{\\bpi}{\\mathbf{\\pi}} \\newcommand{\\bmu}{\\mathbf{\\mu}} \\newcommand{\\bX}{\\mathbf{X}} \\newcommand{\\bZ}{\\mathbf{Z}} \\newcommand{\\bz}{\\mathbf{z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e4e05229ee79b55d6589e1ea8de68f32",
     "grade": false,
     "grade_id": "cell-a0a6fdb7ca694bee",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Installing PyTorch\n",
    "\n",
    "In this lab we will use PyTorch. PyTorch is an open source deep learning framework primarily developed by Facebook's artificial-intelligence research group. In order to install PyTorch in your conda environment go to https://pytorch.org and select your operating system, conda, Python 3.6, no cuda. Copy the text from the \"Run this command:\" box. Now open a terminal and activate your 'ml2labs' conda environment. Paste the text and run. After the installation is done you should restart Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d9c3d77f550b5fd93b34fd18825c47f0",
     "grade": false,
     "grade_id": "cell-746cac8d9a21943b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### MNIST data\n",
    "\n",
    "In this Lab we will use several methods for unsupervised learning on the MNIST dataset of written digits. The dataset contains digital images of handwritten numbers $0$ through $9$. Each image has 28x28 pixels that each take 256 values in a range from white ($= 0$) to  black ($=1$). The labels belonging to the images are also included. \n",
    "Fortunately, PyTorch comes with a MNIST data loader. The first time you run the box below it will download the MNIST data set. That can take a couple of minutes.\n",
    "The main data types in PyTorch are tensors. For Part 1, we will convert those tensors to numpy arrays. In Part 2, we will use the torch module to directly work with PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4fbc152afa1255331d7b88bf00b7156c",
     "grade": false,
     "grade_id": "cell-7c995be0fda080c0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "\n",
    "train_labels = train_dataset.train_labels.numpy()\n",
    "train_data = train_dataset.train_data.numpy()\n",
    "# For EM we will use flattened data\n",
    "train_data = train_data.reshape(train_data.shape[0], -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4fc852f9bfb0bab10d4c23eada309e89",
     "grade": false,
     "grade_id": "cell-8b4a44df532b1867",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 1: Expectation Maximization\n",
    "We will use the Expectation Maximization (EM) algorithm for the recognition of handwritten digits in the MNIST dataset. The images are modelled as a Bernoulli mixture model (see Bishop $\\S9.3.3$):\n",
    "$$\n",
    "p(\\bx|\\bmu, \\bpi) = \\sum_{k=1}^K  \\pi_k \\prod_{i=1}^D \\mu_{ki}^{x_i}(1-\\mu_{ki})^{(1-x_i)}\n",
    "$$\n",
    "where $x_i$ is the value of pixel $i$ in an image, $\\mu_{ki}$ represents the probability that pixel $i$ in class $k$ is black, and $\\{\\pi_1, \\ldots, \\pi_K\\}$ are the mixing coefficients of classes in the data. We want to use this data set to classify new images of handwritten numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "54064637b7e7cf938c0f778d748a226a",
     "grade": false,
     "grade_id": "cell-af03fef663aa85b2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.1 Binary data (5 points)\n",
    "As we like to apply our Bernoulli mixture model, write a function `binarize` to convert the (flattened) MNIST data to binary images, where each pixel $x_i \\in \\{0,1\\}$, by thresholding at an appropriate level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fe8607a4d734f7f26ef1ee1e54b33471",
     "grade": false,
     "grade_id": "cell-ec4365531ca57ef3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'   \\nbin_train_data = binarize(train_data)\\nprint (train_data[0,:])\\nprint (bin_train_data[0,:])\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (train_data.shape)\n",
    "\n",
    "def binarize(X):\n",
    "    # YOUR CODE HERE\n",
    "    X = (X > 256/2).astype(np.float_)\n",
    "    return X\n",
    "'''   \n",
    "bin_train_data = binarize(train_data)\n",
    "print (train_data[0,:])\n",
    "print (bin_train_data[0,:])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "231b2c9f29bc5c536c60cef4d74793a1",
     "grade": true,
     "grade_id": "cell-2f16f57cb68a83b3",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test test test!\n",
    "bin_train_data = binarize(train_data)\n",
    "assert bin_train_data.dtype == np.float\n",
    "assert bin_train_data.shape == train_data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a0a39404cc2f67078b399ee34653a3ac",
     "grade": false,
     "grade_id": "cell-462e747685e8670f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Sample a few images of digits $2$, $3$ and $4$; and show both the original and the binarized image together with their label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3f3c981f0fda5ba3bdfcefb9144305c7",
     "grade": true,
     "grade_id": "cell-784c6bd177a9aa42",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Shuffle\n",
    "p = np.random.permutation(len(train_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4b9da574d24193df76e96ed8ca62c7b0",
     "grade": false,
     "grade_id": "cell-56b33654497d4052",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.2 Implementation (40 points)\n",
    "You are going to write a function ```EM(X, K, max_iter)``` that implements the EM algorithm on the Bernoulli mixture model. \n",
    "\n",
    "The only parameters the function has are:\n",
    "* ```X``` :: (NxD) array of input training images\n",
    "* ```K``` :: size of the latent space\n",
    "* ```max_iter``` :: maximum number of iterations, i.e. one E-step and one M-step\n",
    "\n",
    "You are free to specify your return statement.\n",
    "\n",
    "Make sure you use a sensible way of terminating the iteration process early to prevent unnecessarily running through all epochs. Vectorize computations using ```numpy``` as  much as possible.\n",
    "\n",
    "You should implement the `E_step(X, mu, pi)` and `M_step(X, gamma)` separately in the functions defined below. These you can then use in your function `EM(X, K, max_iter)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "316c9131692747c363b5db8e9091d362",
     "grade": false,
     "grade_id": "cell-882b13c117a73cc4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def E_step(X, mu, pi):\n",
    "    # YOUR CODE HERE\n",
    "    X = np.transpose(X) # Transpose X into 784 by 5\n",
    "    \n",
    "    # Make mu into 10 by 784 by 5\n",
    "    mu = np.repeat(mu[:, :, np.newaxis], X.shape[1], axis=2)\n",
    "    \n",
    "    # Make X into 10 by 784 by 5\n",
    "    X = np.repeat(X[np.newaxis,:,:], mu.shape[0],axis=0)\n",
    "    \n",
    "    # Do what is inside the product\n",
    "    temp1 = np.multiply(np.power(mu,X),np.power(1-mu,1-X))\n",
    "    \n",
    "    # Make it into a 10 by 5 \n",
    "    temp1 = np.prod(temp1,1)\n",
    "    \n",
    "    # Make it into a 5 by 10\n",
    "    temp1 = np.transpose(temp1)\n",
    "    \n",
    "    # Reshape pi\n",
    "    print (X.shape)\n",
    "    pi = np.repeat(pi[np.newaxis,:], X.shape[2], axis = 0)\n",
    "    \n",
    "    # Multiply with pi\n",
    "    temp1 = np.multiply(temp1,pi)\n",
    "    \n",
    "    # Normalize wrt the sum of each row\n",
    "    norm = np.sum(temp1,axis=1)\n",
    "    norm = np.repeat(norm[:,np.newaxis], X.shape[0], axis = 1)\n",
    "    \n",
    "    gamma = np.divide(temp1,norm)\n",
    "    \n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1418f4014e98024fc97446ce27766c1d",
     "grade": true,
     "grade_id": "cell-f7c7dd52d82e2498",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 784, 5)\n"
     ]
    }
   ],
   "source": [
    "# Let's test on 5 datapoints\n",
    "n_test = 5\n",
    "X_test = bin_train_data[:n_test]\n",
    "D_test, K_test = X_test.shape[1], 10\n",
    "\n",
    "np.random.seed(2018)\n",
    "mu_test = np.random.uniform(low=.25, high=.75, size=(K_test,D_test))\n",
    "pi_test = np.ones(K_test) / K_test\n",
    "\n",
    "gamma_test = E_step(X_test, mu_test, pi_test)\n",
    "assert gamma_test.shape == (n_test, K_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2c426a613653174795cd9c8327ab6e20",
     "grade": false,
     "grade_id": "cell-f1b11b8765bd1ef6",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def M_step(X, gamma):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Sum over all the colums, results in something of shape 10 by 1\n",
    "    N_k = np.sum(gamma, axis=0)\n",
    "    pi = N_k / X.shape[0]\n",
    "    \n",
    "    # Make N_k into 10 by 784\n",
    "    N_k = np.repeat(N_k[:,np.newaxis],X.shape[1],axis=1)\n",
    "    \n",
    "    # Calculate x_k and mu\n",
    "    x_k = np.dot(np.transpose(gamma),X)\n",
    "    mu = np.divide(x_k,1.0/N_k)\n",
    "     \n",
    "    return mu, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0f60d48b8b22063cef560b42944a0aa4",
     "grade": true,
     "grade_id": "cell-6e7c751b30acfd45",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Oh, let's test again\n",
    "mu_test, pi_test = M_step(X_test, gamma_test)\n",
    "\n",
    "assert mu_test.shape == (K_test,D_test)\n",
    "assert pi_test.shape == (K_test, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "acfec6384b058cb0ce1932006fbfebc4",
     "grade": true,
     "grade_id": "cell-d6c4368246dee7e6",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def EM(X, K, max_iter, mu=None, pi=None):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Initialize mu and pi first, if necessary:\n",
    "    if mu == None:\n",
    "        mu = np.random.uniform(low=.25, high=.75, size=(K,X.shape[1]))\n",
    "    if pi == None:\n",
    "        pi = np.ones(K) / K\n",
    "        \n",
    "    for e in range(0,max_iter):\n",
    "        gamma = E_step(X, mu, pi)\n",
    "        mu, pi = M_step(X, gamma)\n",
    "    \n",
    "    return mu\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b4fc12faa0da660f7a4d9cc7deb41b25",
     "grade": false,
     "grade_id": "cell-e1077ed3b83489be",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.3 Three digits experiment (10 points)\n",
    "In analogue with Bishop $\\S9.3.3$, sample a training set consisting of only __binary__ images of written digits $2$, $3$, and $4$. Run your EM algorithm and show the reconstructed digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bdbce0fad0ed151063d4c489ce999e3e",
     "grade": true,
     "grade_id": "cell-477155d0264d7259",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 784, 11800)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rubenpolak/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 784, 11800)\n",
      "(3, 784, 11800)\n",
      "(3, 784, 11800)\n",
      "(3, 784, 11800)\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "def create_new_training_set(X,y):\n",
    "    new_X = []\n",
    "    new_y = []\n",
    "    \n",
    "    for i in range(0,len(y)):\n",
    "        if y[i] == 2 or y[1] == 3 or y[i] == 4:\n",
    "            new_X.append(X[i,:])\n",
    "            new_y.append(y[i])\n",
    "            \n",
    "    return np.stack(new_X), np.stack(new_y)\n",
    "\n",
    "new_x,new_y = create_new_training_set(train_data,train_labels)\n",
    "\n",
    "mu = EM(new_x,3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 784)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rubenpolak/miniconda3/lib/python3.6/site-packages/matplotlib/colors.py:823: UserWarning: Warning: converting a masked element to nan.\n",
      "  dtype = np.min_scalar_type(value)\n",
      "/Users/rubenpolak/miniconda3/lib/python3.6/site-packages/numpy/ma/core.py:2809: UserWarning: Warning: converting a masked element to nan.\n",
      "  order=order, subok=True, ndmin=ndmin)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npoxfasr4paaMX2rK+KWm/h9AyFgFyzVQIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11907e390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npoxfasr4paaMX2rK+KWm/h9AyFgFyzVQIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1103cac88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npoxfasr4paaMX2rK+KWm/h9AyFgFyzVQIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x126eeadd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (mu.shape)\n",
    "\n",
    "for i in range(0,mu.shape[0]):\n",
    "    plt.figure(i)\n",
    "    plt.imshow(mu[i,:].reshape(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "485543f4893938d2a9dc1c17d8221cbc",
     "grade": false,
     "grade_id": "cell-88c9664f995b1909",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Can you identify which element in the latent space corresponds to which digit? What are the identified mixing coefficients for digits $2$, $3$ and $4$, and how do these compare to the true ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ae7b5acea6089e2590059f90b0d0a0be",
     "grade": true,
     "grade_id": "cell-3680ae2159c48193",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "98e04feb59a36867367b3027df9e226d",
     "grade": false,
     "grade_id": "cell-0891dda1c3e80e9a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.4 Experiments (20 points)\n",
    "Perform the follow-up experiments listed below using your implementation of the EM algorithm. For each of these, describe/comment on the obtained results and give an explanation. You may still use your dataset with only digits 2, 3 and 4 as otherwise computations can take very long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "439067186fa3ef1d7261a9bcf5a84ea6",
     "grade": false,
     "grade_id": "cell-06fe1b1355689928",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.4.1 Size of the latent space (5 points)\n",
    "Run EM with $K$ larger or smaller than the true number of classes. Describe your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "791512aeadd30c4b586b966ca10e6fad",
     "grade": true,
     "grade_id": "cell-6c9057f2546b7215",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-15b94d1fa268>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e12e40c2d2165e3bb500b5504128910d",
     "grade": true,
     "grade_id": "cell-f01c37653160244b",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b306681523a2e35eea310ac10bb68999",
     "grade": false,
     "grade_id": "cell-cf478d67239b7f2e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.4.2 Identify misclassifications (10 points)\n",
    "How can you use the data labels to assign a label to each of the clusters/latent variables? Use this to identify images that are 'misclassified' and try to understand why they are. Report your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "000c11bd8756a4e24296c7c55d3ee17e",
     "grade": true,
     "grade_id": "cell-daa1a492fbba5c7e",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "baf43434481c13d76ad51e3ba07e2bf5",
     "grade": true,
     "grade_id": "cell-329245c02df7850d",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "640bc57a2d08c3becf534bb5e4b35971",
     "grade": false,
     "grade_id": "cell-67ce1222e8a7837b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.4.3 Initialize with true values (5 points)\n",
    "Initialize the three classes with the true values of the parameters and see what happens. Report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a48f788e286458ef0f776865a3bcd58b",
     "grade": true,
     "grade_id": "cell-aa5d6b9f941d985d",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1dc4adf3081f3bec93f94c3b12b87db9",
     "grade": true,
     "grade_id": "cell-981e44f35a3764b0",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bd613f41e5d2b7d22b0d5b1e7644a48a",
     "grade": false,
     "grade_id": "cell-19bfd7cf4017ed84",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 2: Variational Auto-Encoder\n",
    "\n",
    "A Variational Auto-Encoder (VAE) is a probabilistic model $p(\\bx, \\bz)$ over observed variables $\\bx$ and latent variables and/or parameters $\\bz$. Here we distinguish the decoder part, $p(\\bx | \\bz) p(\\bz)$ and an encoder part $p(\\bz | \\bx)$ that are both specified with a neural network. A lower bound on the log marginal likelihood $\\log p(\\bx)$ can be obtained by approximately inferring the latent variables z from the observed data x using an encoder distribution $q(\\bz| \\bx)$ that is also specified as a neural network. This lower bound is then optimized to fit the model to the data. \n",
    "\n",
    "The model was introduced by Diederik Kingma (during his PhD at the UVA) and Max Welling in 2013, https://arxiv.org/abs/1312.6114. \n",
    "\n",
    "Since it is such an important model there are plenty of well written tutorials that should help you with the assignment. E.g: https://jaan.io/what-is-variational-autoencoder-vae-tutorial/.\n",
    "\n",
    "In the following, we will make heavily use of the torch module, https://pytorch.org/docs/stable/index.html. Most of the time replacing `np.` with `torch.` will do the trick, e.g. `np.sum` becomes `torch.sum` and `np.log` becomes `torch.log`. In addition, we will use `torch.FloatTensor()` as an equivalent to `np.array()`. In order to train our VAE efficiently we will make use of batching. The number of data points in a batch will become the first dimension of our data tensor, e.g. A batch of 128 MNIST images has the dimensions [128, 1, 28, 28]. To check check the dimensions of a tensor you can call `.size()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "92bd337f41c3f94777f47376c7149ca7",
     "grade": false,
     "grade_id": "cell-bcbe35b20c1007d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.1 Loss function\n",
    "The objective function (variational lower bound), that we will use to train the VAE, consists of two terms: a log Bernoulli loss (reconstruction loss) and a Kullback–Leibler divergence. We implement the two terms separately and combine them in the end.\n",
    "As seen in Part 1: Expectation Maximization, we can use a multivariate Bernoulli distribution to model the likelihood $p(\\bx | \\bz)$ of black and white images. Formally, the variational lower bound is maximized but in PyTorch we are always minimizing therefore we need to calculate the negative log Bernoulli loss and Kullback–Leibler divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3fb5f70b132e1233983ef89d19998374",
     "grade": false,
     "grade_id": "cell-389d81024af846e5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.1.1 Negative Log Bernoulli loss (5 points)\n",
    "The negative log Bernoulli loss is defined as,\n",
    "\n",
    "\\begin{align}\n",
    "loss = - (\\sum_i^D \\bx_i \\log \\hat{\\bx_i} + (1 − \\bx_i) \\log(1 − \\hat{\\bx_i})).\n",
    "\\end{align}\n",
    "\n",
    "Write a function `log_bernoulli_loss` that takes a D dimensional vector `x`, its reconstruction `x_hat` and returns the negative log Bernoulli loss. Make sure that your function works for batches of arbitrary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "952435ca03f47ab67a7e88b8306fc9a0",
     "grade": false,
     "grade_id": "cell-1d504606d6f99145",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def log_bernoulli_loss(x_hat, x):\n",
    "    \n",
    "    if type(x) != type(torch.ones(1)):\n",
    "        x = x.data\n",
    "    \n",
    "    lb_loss = nn.functional.binary_cross_entropy(x, x_hat)\n",
    "    \n",
    "    return lb_loss.data.numpy()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bd2a490aa694507bd032e86d77fc0087",
     "grade": true,
     "grade_id": "cell-9666dad0b2a9f483",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Test test test\n",
    "x_test = torch.FloatTensor([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8], [0.9, 0.9, 0.9, 0.9]])\n",
    "x_hat_test = torch.FloatTensor([[0.11, 0.22, 0.33, 0.44], [0.55, 0.66, 0.77, 0.88], [0.99, 0.99, 0.99, 0.99]])\n",
    "\n",
    "assert log_bernoulli_loss(x_hat_test, x_test) > 0.0\n",
    "assert log_bernoulli_loss(x_hat_test, x_test) < 10.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6b75b7a531ecc87bce57925c4da464ee",
     "grade": false,
     "grade_id": "cell-b3a7c02dee7aa505",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.1.2 Negative Kullback–Leibler divergence (10 Points)\n",
    "The variational lower bound (the objective to be maximized) contains a KL term $D_{KL}(q(\\bz)||p(\\bz))$ that can often be calculated analytically. In the VAE we assume $q = N(\\bz, \\mu, \\sigma^2I)$ and $p = N(\\bz, 0, I)$. Solve analytically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d01a7e7fe2dcf5f1c5fb955b85c8a04a",
     "grade": true,
     "grade_id": "cell-4cab10fd1a636858",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "328115c94a66e8aba0a62896e647c3ba",
     "grade": false,
     "grade_id": "cell-c49899cbf2a49362",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Write a function `KL_loss` that takes two J dimensional vectors `mu` and `logvar` and returns the negative Kullback–Leibler divergence. Where `logvar` is $\\log(\\sigma^2)$. Make sure that your function works for batches of arbitrary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "33b14b79372dd0235d67bb66921cd3e0",
     "grade": false,
     "grade_id": "cell-125b41878005206b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def KL_loss(mu, logvar):\n",
    "    return torch.sum(mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)) * -.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cf72e196d2b60827e8e940681ac50a07",
     "grade": true,
     "grade_id": "cell-ba714bbe270a3f39",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Test test test\n",
    "mu_test = torch.FloatTensor([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
    "logvar_test = torch.FloatTensor([[0.01, 0.02], [0.03, 0.04], [0.05, 0.06]])\n",
    "\n",
    "assert KL_loss(mu_test, logvar_test) > 0.0\n",
    "assert KL_loss(mu_test, logvar_test) < 10.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "65335a588baac26bc48dd6c4d275fdca",
     "grade": false,
     "grade_id": "cell-18cb3f8031edec23",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.1.3 Putting the losses together (5 points)\n",
    "Write a function `loss_function` that takes a D dimensional vector `x`, its reconstruction `x_hat`, two J dimensional vectors `mu` and `logvar` and returns the final loss. Make sure that your function works for batches of arbitrary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f6ecb5b60b2c8d7b90070ed59320ee70",
     "grade": false,
     "grade_id": "cell-d2d18781683f1302",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "mse_loss = nn.MSELoss(size_average=False)\n",
    "\n",
    "def loss_function(x_hat, x, mu, logvar):\n",
    "    \n",
    "    xent_loss = log_bernoulli_loss(x_hat, x) + KL_loss(mu, logvar)\n",
    "    \n",
    "    return xent_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "816e9508408bfcb2c7332b508d505081",
     "grade": true,
     "grade_id": "cell-57747988d29bbb5d",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.96963584]\n",
      "[ 0.96963584]\n"
     ]
    }
   ],
   "source": [
    "x_test = torch.FloatTensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\n",
    "x_hat_test = torch.FloatTensor([[0.11, 0.22, 0.33], [0.44, 0.55, 0.66], [0.77, 0.88, 0.99]])\n",
    "mu_test = torch.FloatTensor([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
    "logvar_test = torch.FloatTensor([[0.01, 0.02], [0.03, 0.04], [0.05, 0.06]])\n",
    "\n",
    "assert loss_function(x_hat_test, x_test, mu_test, logvar_test) > 0.0\n",
    "assert loss_function(x_hat_test, x_test, mu_test, logvar_test) < 10.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4506e06ed44a0535140582277a528ba4",
     "grade": false,
     "grade_id": "cell-9e3ba708967fe918",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.2 The model\n",
    "Below you see a data structure for the VAE. The modell itself consists of two main parts the encoder (images $\\bx$ to latent variables $\\bz$) and the decoder (latent variables $\\bz$ to images $\\bx$). The encoder is using 3 fully-connected layers, whereas the decoder is using fully-connected layers. Right now the data structure is quite empty, step by step will update its functionality. For test purposes we will initialize a VAE for you. After the data structure is completed you will do the hyperparameter search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "31eccf2f6600764e28eb4bc6c5634e49",
     "grade": false,
     "grade_id": "cell-e7d9dafee18f28a1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F \n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, fc1_dims, fc21_dims, fc22_dims, fc3_dims, fc4_dims):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(*fc1_dims)\n",
    "        self.fc21 = nn.Linear(*fc21_dims)\n",
    "        self.fc22 = nn.Linear(*fc22_dims)\n",
    "        self.fc3 = nn.Linear(*fc3_dims)\n",
    "        self.fc4 = nn.Linear(*fc4_dims)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # To be implemented\n",
    "        raise Exception('Method not implemented')\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # To be implemented\n",
    "        raise Exception('Method not implemented')\n",
    "\n",
    "    def decode(self, z):\n",
    "        # To be implemented\n",
    "        raise Exception('Method not implemented')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # To be implemented\n",
    "        raise Exception('Method not implemented')\n",
    "\n",
    "VAE_test = VAE(fc1_dims=(784, 4), fc21_dims=(4, 2), fc22_dims=(4, 2), fc3_dims=(2, 4), fc4_dims=(4, 784))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1a2243397998b4f55c25dfd734f3e7e0",
     "grade": false,
     "grade_id": "cell-c4f9e841b8972a43",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.3 Encoding (10 points)\n",
    "Write a function `encode` that gets a vector `x` with 784 elements (flattened MNIST image) and returns `mu` and `logvar`. Your function should use three fully-connected layers (`self.fc1()`, `self.fc21()`, `self.fc22()`). First, you should use `self.fc1()` to embed `x`. Second, you should use `self.fc21()` and `self.fc22()` on the embedding of `x` to compute `mu` and `logvar` respectively. PyTorch comes with a variety of activation functions, the most common calls are `F.relu()`, `F.sigmoid()`, `F.tanh()`. Make sure that your function works for batches of arbitrary size.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "628bcd88c611cf01e70f77854600199b",
     "grade": false,
     "grade_id": "cell-93cb75b98ae76569",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def encode(self, x):\n",
    "    \n",
    "    if type(x) != type(torch.ones(1)):\n",
    "        x = x.data\n",
    "        \n",
    "    x = self.fc1(Variable(x))\n",
    "    x = F.relu(x)\n",
    "    mu, logvar = self.fc21(x), self.fc22(x)\n",
    "    return mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "370d930fa9f10f1d3a451f3805c04d88",
     "grade": true,
     "grade_id": "cell-9648960b73337a70",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Test, test, test\n",
    "VAE.encode = encode\n",
    "\n",
    "x_test = torch.ones((5,784))\n",
    "mu_test, logvar_test = VAE_test.encode(x_test)\n",
    "\n",
    "assert np.allclose(mu_test.size(), [5, 2])\n",
    "assert np.allclose(logvar_test.size(), [5, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6f597cc2b5ef941af282d7162297f865",
     "grade": false,
     "grade_id": "cell-581b4ed1996be868",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.4 Reparameterization (10 points)\n",
    "One of the major question that the VAE is answering, is 'how to take derivatives with respect to the parameters of a stochastic variable?', i.e. if we are given $\\bz$ that is drawn from a distribution $q(\\bz|\\bx)$, and we want to take derivatives. This step is necessary to be able to use gradient-based optimization algorithms like SGD.\n",
    "For some distributions, it is possible to reparameterize samples in a clever way, such that the stochasticity is independent of the parameters. We want our samples to deterministically depend on the parameters of the distribution. For example, in a normally-distributed variable with mean $\\mu$ and standard deviation $\\sigma$, we can sample from it like this:\n",
    "\n",
    "\\begin{align}\n",
    "\\bz = \\mu + \\sigma \\odot \\epsilon,\n",
    "\\end{align}\n",
    "\n",
    "where $\\odot$ is the element-wise multiplication and $\\epsilon$ is sampled from $N(0, I)$.\n",
    "\n",
    "\n",
    "Write a function `reparameterize` that takes two J dimensional vectors `mu` and `logvar`. It should return $\\bz = \\mu + \\sigma \\odot \\epsilon$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6331cb5dd23aaacbcf1a52cfecb1afaa",
     "grade": false,
     "grade_id": "cell-679aea8b2adf7ec4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def reparameterize(self, mu, logvar):\n",
    "    \n",
    "    # std = exp(log_variance / 2)\n",
    "    std = logvar.mul(0.5).exp_()\n",
    "\n",
    "    # Random normal vector depending on std size\n",
    "    epsilon = torch.FloatTensor(std.size()).normal_()\n",
    "\n",
    "    sample = mu + std * epsilon\n",
    "    \n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "38d4e047717ab334b262c8c177f0a420",
     "grade": true,
     "grade_id": "cell-fdd7b27a3d17f84e",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Test, test, test\n",
    "VAE.reparameterize = reparameterize\n",
    "VAE_test.train()\n",
    "\n",
    "mu_test = torch.FloatTensor([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
    "logvar_test = torch.FloatTensor([[0.01, 0.02], [0.03, 0.04], [0.05, 0.06]])\n",
    "\n",
    "z_test = VAE_test.reparameterize(mu_test, logvar_test)\n",
    "\n",
    "assert np.allclose(z_test.size(), [3, 2])\n",
    "assert z_test[0][0] < 5.0\n",
    "assert z_test[0][0] > -5.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9241ab0eaf8366c37ad57072ce66f095",
     "grade": false,
     "grade_id": "cell-0be851f9f7f0a93e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.5 Decoding (10 points)\n",
    "Write a function `decode` that gets a vector `z` with J elements and returns a vector `x_hat` with 784 elements (flattened MNIST image). Your function should use two fully-connected layers (`self.fc3()`, `self.fc4()`). PyTorch comes with a variety of activation functions, the most common calls are `F.relu()`, `F.sigmoid()`, `F.tanh()`. Make sure that your function works for batches of arbitrary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e8e833cfd7c54a9b67a38056d5d6cab8",
     "grade": false,
     "grade_id": "cell-bf92bb3878275a41",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def decode(self, z):\n",
    "    \n",
    "    x_hat = self.fc3(Variable(z))\n",
    "    x_hat = F.relu(x_hat)\n",
    "    x_hat = self.fc4(x_hat)\n",
    "    x_hat = F.sigmoid(x_hat)\n",
    "    \n",
    "    return x_hat.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7732293fd7d971fcf255496e8c68638d",
     "grade": true,
     "grade_id": "cell-4abb91cb9e80af5d",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# test test test\n",
    "VAE.decode = decode\n",
    "\n",
    "z_test = torch.ones((5,2))\n",
    "x_hat_test = VAE_test.decode(z_test)\n",
    "\n",
    "assert np.allclose(x_hat_test.size(), [5, 784])\n",
    "assert (x_hat_test <= 1).all()\n",
    "assert (x_hat_test >= 0).all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e2e113d1f45398b2a1399c336526e755",
     "grade": false,
     "grade_id": "cell-97511fbc4f5b469b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.6 Forward pass (10)\n",
    "To complete the data structure you have to define a forward pass through the VAE. A single forward pass consists of the encoding of an MNIST image $\\bx$ into latent space $\\bz$, the reparameterization of $\\bz$ and the decoding of $\\bz$ into an image $\\bx$.\n",
    "\n",
    "Write a function `forward` that gets a a vector `x` with 784 elements (flattened MNIST image) and returns a vector `x_hat` with 784 elements (flattened MNIST image), `mu` and `logvar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8b7433c4631dd01c07a5fe287e55ae13",
     "grade": false,
     "grade_id": "cell-26bb463b9f98ebd5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    x = x.view(-1, 784)\n",
    "    \n",
    "    if x.type:\n",
    "        print(x.type)\n",
    "    \n",
    "    mu, logvar = self.encode(x)\n",
    "    z = self.reparameterize(mu.data, logvar.data)\n",
    "    x_hat = self.decode(z)\n",
    "    \n",
    "    return x_hat, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8e7e495f40465c162512e9873c360b25",
     "grade": true,
     "grade_id": "cell-347e5fba3d02754b",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method _type of \n",
      "    1     1     1  ...      1     1     1\n",
      "    1     1     1  ...      1     1     1\n",
      "    1     1     1  ...      1     1     1\n",
      "    1     1     1  ...      1     1     1\n",
      "    1     1     1  ...      1     1     1\n",
      "[torch.FloatTensor of size 5x784]\n",
      ">\n"
     ]
    }
   ],
   "source": [
    "# test test test \n",
    "VAE.forward = forward\n",
    "\n",
    "x_test = torch.ones((5,784))\n",
    "x_hat_test, mu_test, logvar_test = VAE_test.forward(x_test)\n",
    "\n",
    "assert np.allclose(x_hat_test.size(), [5, 784])\n",
    "assert np.allclose(mu_test.size(), [5, 2])\n",
    "assert np.allclose(logvar_test.size(), [5, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a114a6fd781fb949b887e6a028e07946",
     "grade": false,
     "grade_id": "cell-62c89e4d3b253671",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.7 Training (15)\n",
    "We will now train the VAE using an optimizer called Adam, https://arxiv.org/abs/1412.6980. The code to train a model in PyTorch is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f3b6bb965fb48229c63cacda48baea65",
     "grade": false,
     "grade_id": "cell-be75f61b09f3b9b6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def train(epoch, train_loader, model, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data.view(-1, 784), mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "48ca730dbef06a668f4dfdb24888f265",
     "grade": false,
     "grade_id": "cell-da1b063b7de850b9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's train. You have to choose the hyperparameters. Make sure your loss is going down in a reasonable amount of epochs (around 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "846430258fb80f50b161135448726520",
     "grade": false,
     "grade_id": "cell-d4d4408d397f6967",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "fc1_dims = (784, 4)\n",
    "fc21_dims = (4, 2)\n",
    "fc22_dims = (4, 2)\n",
    "fc3_dims = (2, 4)\n",
    "fc4_dims = (4, 784)\n",
    "lr = 1e-4\n",
    "batch_size = 50\n",
    "epochs = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b93390f399b743276bc25e67493344f2",
     "grade": true,
     "grade_id": "cell-ca352d8389c1809a",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell contains a hidden test, please don't delete it, thx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "20719070ed85964de9722acc3456a515",
     "grade": false,
     "grade_id": "cell-5c77370db7cec9f2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the box below to train the model using the hyperparameters you entered above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "38306be3638e85812bd5b2a052fcc0a4",
     "grade": false,
     "grade_id": "cell-5712d42de1068398",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Variable.type of Variable containing:\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "          ...             ⋱             ...          \n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 50x784]\n",
      ">\n",
      "[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[Variable containing:\n",
      " 16.5172\n",
      "[torch.FloatTensor of size 1]\n",
      "]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-523-601b0aa70f1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVAE_MNIST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-520-be1f04d67f7e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, optimizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "\n",
    "# Load data\n",
    "train_data = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=batch_size, shuffle=True, **{})\n",
    "\n",
    "# Init model\n",
    "VAE_MNIST = VAE(fc1_dims=fc1_dims, fc21_dims=fc21_dims, fc22_dims=fc22_dims, fc3_dims=fc3_dims, fc4_dims=fc4_dims)\n",
    "\n",
    "# Init optimizer\n",
    "optimizer = optim.Adam(VAE_MNIST.parameters(), lr=lr)\n",
    "\n",
    "# Train\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch, train_loader, VAE_MNIST, optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e2f8fcc9384e30cb154cf931f223898b",
     "grade": false,
     "grade_id": "cell-bd07c058c661b9c6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the box below to check if the model you trained above is able to correctly reconstruct images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "80d198e03b1287741d761a12e38dcf73",
     "grade": false,
     "grade_id": "cell-df03d717307a6863",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method _type of \n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 30 to 39 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 40 to 49 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 50 to 59 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 60 to 69 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 70 to 79 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 80 to 89 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 90 to 99 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 100 to 109 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 110 to 119 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 120 to 129 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 130 to 139 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 140 to 149 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 150 to 159 \n",
      " 0.0000  0.0000  0.0118  0.0706  0.0706  0.0706  0.4941  0.5333  0.6863  0.1020\n",
      "\n",
      "Columns 160 to 169 \n",
      " 0.6510  1.0000  0.9686  0.4980  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 170 to 179 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.1176  0.1412  0.3686  0.6039\n",
      "\n",
      "Columns 180 to 189 \n",
      " 0.6667  0.9922  0.9922  0.9922  0.9922  0.9922  0.8824  0.6745  0.9922  0.9490\n",
      "\n",
      "Columns 190 to 199 \n",
      " 0.7647  0.2510  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 200 to 209 \n",
      " 0.0000  0.0000  0.0000  0.1922  0.9333  0.9922  0.9922  0.9922  0.9922  0.9922\n",
      "\n",
      "Columns 210 to 219 \n",
      " 0.9922  0.9922  0.9922  0.9843  0.3647  0.3216  0.3216  0.2196  0.1529  0.0000\n",
      "\n",
      "Columns 220 to 229 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 230 to 239 \n",
      " 0.0000  0.0706  0.8588  0.9922  0.9922  0.9922  0.9922  0.9922  0.7765  0.7137\n",
      "\n",
      "Columns 240 to 249 \n",
      " 0.9686  0.9451  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 250 to 259 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 260 to 269 \n",
      " 0.3137  0.6118  0.4196  0.9922  0.9922  0.8039  0.0431  0.0000  0.1686  0.6039\n",
      "\n",
      "Columns 270 to 279 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 280 to 289 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0549\n",
      "\n",
      "Columns 290 to 299 \n",
      " 0.0039  0.6039  0.9922  0.3529  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 300 to 309 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 310 to 319 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.5451\n",
      "\n",
      "Columns 320 to 329 \n",
      " 0.9922  0.7451  0.0078  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 330 to 339 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 340 to 349 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0431  0.7451  0.9922\n",
      "\n",
      "Columns 350 to 359 \n",
      " 0.2745  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 360 to 369 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 370 to 379 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.1373  0.9451  0.8824  0.6275\n",
      "\n",
      "Columns 380 to 389 \n",
      " 0.4235  0.0039  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 390 to 399 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 400 to 409 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.3176  0.9412  0.9922  0.9922  0.4667\n",
      "\n",
      "Columns 410 to 419 \n",
      " 0.0980  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 420 to 429 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 430 to 439 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.1765  0.7294  0.9922  0.9922  0.5882  0.1059\n",
      "\n",
      "Columns 440 to 449 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 450 to 459 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 460 to 469 \n",
      " 0.0000  0.0000  0.0000  0.0627  0.3647  0.9882  0.9922  0.7333  0.0000  0.0000\n",
      "\n",
      "Columns 470 to 479 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 480 to 489 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 490 to 499 \n",
      " 0.0000  0.0000  0.0000  0.9765  0.9922  0.9765  0.2510  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 500 to 509 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 510 to 519 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.1804  0.5098\n",
      "\n",
      "Columns 520 to 529 \n",
      " 0.7176  0.9922  0.9922  0.8118  0.0078  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 530 to 539 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 540 to 549 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.1529  0.5804  0.8980  0.9922  0.9922  0.9922\n",
      "\n",
      "Columns 550 to 559 \n",
      " 0.9804  0.7137  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 560 to 569 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 570 to 579 \n",
      " 0.0941  0.4471  0.8667  0.9922  0.9922  0.9922  0.9922  0.7882  0.3059  0.0000\n",
      "\n",
      "Columns 580 to 589 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 590 to 599 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0902  0.2588  0.8353  0.9922\n",
      "\n",
      "Columns 600 to 609 \n",
      " 0.9922  0.9922  0.9922  0.7765  0.3176  0.0078  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 610 to 619 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 620 to 629 \n",
      " 0.0000  0.0000  0.0706  0.6706  0.8588  0.9922  0.9922  0.9922  0.9922  0.7647\n",
      "\n",
      "Columns 630 to 639 \n",
      " 0.3137  0.0353  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 640 to 649 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.2157  0.6745\n",
      "\n",
      "Columns 650 to 659 \n",
      " 0.8863  0.9922  0.9922  0.9922  0.9922  0.9569  0.5216  0.0431  0.0000  0.0000\n",
      "\n",
      "Columns 660 to 669 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 670 to 679 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.5333  0.9922  0.9922  0.9922\n",
      "\n",
      "Columns 680 to 689 \n",
      " 0.8314  0.5294  0.5176  0.0627  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 690 to 699 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 700 to 709 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 710 to 719 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 720 to 729 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 730 to 739 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 740 to 749 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 750 to 759 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 760 to 769 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 770 to 779 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 780 to 783 \n",
      " 0.0000  0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 1x784]\n",
      ">\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cannot call .data on a torch.Tensor: did you intend to use autograd.Variable?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-359-9748eeae0334>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader_plot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mx_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE_MNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%i'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cannot call .data on a torch.Tensor: did you intend to use autograd.Variable?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot call .data on a torch.Tensor: did you intend to use autograd.Variable?"
     ]
    }
   ],
   "source": [
    "### Let's check if the reconstructions make sense\n",
    "# Set model to test mode\n",
    "VAE_MNIST.eval()\n",
    "    \n",
    "# Reconstructed\n",
    "train_data_plot = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor())\n",
    "\n",
    "train_loader_plot = torch.utils.data.DataLoader(train_data_plot,\n",
    "                                           batch_size=1, shuffle=False, **{})\n",
    "\n",
    "for batch_idx, (data, _) in enumerate(train_loader_plot):\n",
    "    x_hat, mu, logvar = VAE_MNIST(data)\n",
    "    plt.imshow(x_hat.view(1,28,28).squeeze().data.numpy(), cmap='gray')\n",
    "    plt.title('%i' % train_data.train_labels[batch_idx])\n",
    "    plt.show()\n",
    "    if batch_idx == 3:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7f559122b150f5f1228d6b66b62f462c",
     "grade": false,
     "grade_id": "cell-76649d51fdf133dc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.8 Visualize latent space (20 points)\n",
    "Now, implement the auto-encoder now with a 2-dimensional latent space, and train again over the MNIST data. Make a visualization of the learned manifold by using a linearly spaced coordinate grid as input for the latent space, as seen in  https://arxiv.org/abs/1312.6114 Figure 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c879ffdb0d355349d7144a33d16ca93a",
     "grade": true,
     "grade_id": "cell-4a0af6d08d055bee",
     "locked": false,
     "points": 20,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-360-15b94d1fa268>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b9eb1684d646eea84a25638d184bfbda",
     "grade": false,
     "grade_id": "cell-dc5e1247a1e21009",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.8 Amortized inference (10 points)\n",
    "What is amortized inference? Where in the code of Part 2 is it used? What is the benefit of using it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "364ed922da59070f319d0bdfb0e41d92",
     "grade": true,
     "grade_id": "cell-6f7808a9b0098dbf",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
